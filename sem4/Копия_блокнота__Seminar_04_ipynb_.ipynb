{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "Копия блокнота \"Seminar-04.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzzr_qkrm6j_",
        "colab_type": "text"
      },
      "source": [
        "# Семинар 4: Deep Deterministic Policy Gradient\n",
        "\n",
        "На этом семинаре мы будем обучать нейронную сеть на фреймворке __pytorch__ с помощью алгоритма Deep Deterministic Policy Gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i91bu8oNm6kB",
        "colab_type": "text"
      },
      "source": [
        "## Теория\n",
        "\n",
        "Deep Deterministic Policy Gradient (DDPG) - это алгоритм, который одновременно учит Q-функцию и стратегию. Он использует off-policy данные и уравнения Беллмана для обучения Q-функции, а Q-функция используется для обучения стратегии.\n",
        "\n",
        "Данный подход тесно связан с Q-обучением и мотивирован следующей идеей: если вы знаете оптимальную функцию action-value $Q^*(s,a)$, тогда для конкретного состояния, оптимальное действие $a^*(s)$ может быть найдено решением: \n",
        "\n",
        "$$a^*(s) = \\arg \\max_a Q^*(s,a).$$\n",
        "\n",
        "Для сред с дискретным пространством действий - это легко, вычисляем полезности для каждого из действий, а потом берем максимум. Для непрерывных действий - это сложная оптимизационная задача.\n",
        "\n",
        "DDPG чередует обучение аппроксиматора $Q^*(s,a)$ с обучением аппроксиматора  $a^*(s)$, и делает это специальным образом именно для непрерывных (continuous) сред, что отражается в том как алгоритм вычисляет $\\max_a Q^*(s,a)$.\n",
        "Поскольку пространство действий непрерывно, предполагается, что функция $Q^*(s,a)$ дифференцируема по аргументу действия. Это позволяет нам установить эффективное правило обучения на основе градиента для стратегии $\\mu(s)$.\n",
        "\n",
        "<img src=\"https://spinningup.openai.com/en/latest/_images/math/5811066e89799e65be299ec407846103fcf1f746.svg\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7HCP3A0m6kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro8EBlS9m6kG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT5q086Zm6kI",
        "colab_type": "text"
      },
      "source": [
        "## Environment\n",
        "### Нормализация пространства действий"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVb5M4uUm6kJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NormalizedActions(gym.ActionWrapper):\n",
        "\n",
        "    def action(self, action):\n",
        "        low_bound   = self.action_space.low\n",
        "        upper_bound = self.action_space.high\n",
        "        \n",
        "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
        "        action = np.clip(action, low_bound, upper_bound)\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def reverse_action(self, action):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g0YCRtqm6kL",
        "colab_type": "text"
      },
      "source": [
        "### Исследование - GaussNoise\n",
        "#### Добавляем Гауссовский шум к действиям детерминированной стратегии"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Zky62Om6kL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GaussNoise:\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def get_action(self, action):\n",
        "        noisy_action = np.random.normal(action, self.sigma)\n",
        "        return noisy_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxMEuwZmm6kN",
        "colab_type": "text"
      },
      "source": [
        "<h2>Оригинальная статья: Continuous control with deep reinforcement learning <a href=\"https://arxiv.org/abs/1509.02971\">Arxiv</a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXAvd1V3m6kO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_inputs, \n",
        "        num_actions, \n",
        "        hidden_size, \n",
        "        init_w=3e-3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(num_inputs + num_actions, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], 1)\n",
        "        x = self.net(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_inputs, \n",
        "        num_actions, \n",
        "        hidden_size, \n",
        "        init_w=3e-3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_size, num_actions)\n",
        "        \n",
        "        self.head.weight.data.uniform_(-init_w, init_w)\n",
        "        self.head.bias.data.uniform_(-init_w, init_w)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        x = state\n",
        "        x = self.net(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state  = torch.tensor(state, dtype=torch.float32)\\\n",
        "            .unsqueeze(0).to(device)\n",
        "        action = self.forward(state)\n",
        "        action = action.detach().cpu().numpy()[0]\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etF52eRgm6kQ",
        "colab_type": "text"
      },
      "source": [
        "<h2>DDPG обновление</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80Fu0BkZm6kQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ddpg_update(\n",
        "    state, \n",
        "    action, \n",
        "    reward, \n",
        "    next_state, \n",
        "    done, \n",
        "    gamma = 0.99,\n",
        "    min_value=-np.inf,\n",
        "    max_value=np.inf,\n",
        "    soft_tau=1e-2,\n",
        "):  \n",
        "    state      = torch.tensor(state, dtype=torch.float32).to(device)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
        "    action     = torch.tensor(action, dtype=torch.float32).to(device)\n",
        "    reward     = torch.tensor(reward, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "    done       = torch.tensor(np.float32(done)).unsqueeze(1).to(device)\n",
        "\n",
        "    policy_loss = value_net(state, policy_net(state))\n",
        "    policy_loss = -policy_loss.mean()\n",
        "\n",
        "    next_action    = target_policy_net(next_state)\n",
        "    target_value   = target_value_net(next_state, next_action.detach())\n",
        "    expected_value = reward + (1.0 - done) * gamma * target_value\n",
        "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
        "\n",
        "    value = value_net(state, action)\n",
        "    value_loss = value_criterion(value, expected_value.detach())\n",
        "\n",
        "\n",
        "    policy_optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    value_optimizer.zero_grad()\n",
        "    value_loss.backward()\n",
        "    value_optimizer.step()\n",
        "\n",
        "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
        "            target_param.data.copy_(\n",
        "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
        "            )\n",
        "\n",
        "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
        "            target_param.data.copy_(\n",
        "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxUA9k51m6kS",
        "colab_type": "text"
      },
      "source": [
        "## Память повторов (replay buffer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUkNsIfbm6kU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5I6fYV8m6kW",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVUZfxxFm6kW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size  = 128\n",
        "\n",
        "def generate_session(t_max=1000, train=False):\n",
        "    \"\"\"эпизод взаимодействие агента со средой, а также вызов процесса обучения\"\"\"\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        action = policy_net.get_action(state)\n",
        "        if train:\n",
        "            action = noise.get_action(action)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if train:\n",
        "            replay_buffer.push(state, action, reward, next_state, done)\n",
        "            if len(replay_buffer) > batch_size:\n",
        "                states, actions, rewards, next_states, dones = \\\n",
        "                    replay_buffer.sample(batch_size)\n",
        "                ddpg_update(states, actions, rewards, next_states, dones)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqkh2WTmm6kY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = NormalizedActions(gym.make(\"Pendulum-v0\"))\n",
        "noise = GaussNoise(sigma=0.3)\n",
        "\n",
        "state_dim  = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "hidden_dim = 256\n",
        "\n",
        "value_net  = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
        "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
        "\n",
        "target_value_net  = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
        "target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
        "\n",
        "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
        "    target_param.data.copy_(param.data)\n",
        "\n",
        "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
        "    target_param.data.copy_(param.data)\n",
        "    \n",
        "    \n",
        "value_lr  = 1e-3\n",
        "policy_lr = 1e-4\n",
        "\n",
        "value_optimizer  = optim.Adam(value_net.parameters(),  lr=value_lr)\n",
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
        "\n",
        "value_criterion = nn.MSELoss()\n",
        "\n",
        "replay_buffer_size = 10000\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "kYADAYW4m6ka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "085946c1-f75e-48ec-e887-1cb6eedb0780"
      },
      "source": [
        "max_steps   = 500\n",
        "\n",
        "valid_mean_rewards = []\n",
        "for i in range(100):    \n",
        "    session_rewards_train = [\n",
        "        generate_session(t_max=max_steps, train=True) \n",
        "        for _ in range(10)\n",
        "    ]\n",
        "    session_rewards_valid = [\n",
        "        generate_session(t_max=max_steps, train=False) \n",
        "        for _ in range(10)\n",
        "    ]\n",
        "    print(\n",
        "        \"epoch #{:02d}\\tmean reward (train) = {:.3f}\\tmean reward (valid) = {:.3f}\".format(\n",
        "        i, np.mean(session_rewards_train), np.mean(session_rewards_valid))\n",
        "    )\n",
        "\n",
        "    valid_mean_rewards.append(np.mean(session_rewards_valid))\n",
        "    if len(valid_mean_rewards) > 5 and np.mean(valid_mean_rewards[-5:]) > -200:\n",
        "        print(\"Pendulum принят!\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #00\tmean reward (train) = -1326.031\tmean reward (valid) = -1595.047\n",
            "epoch #01\tmean reward (train) = -1196.696\tmean reward (valid) = -1268.831\n",
            "epoch #02\tmean reward (train) = -264.864\tmean reward (valid) = -145.502\n",
            "epoch #03\tmean reward (train) = -155.861\tmean reward (valid) = -132.042\n",
            "epoch #04\tmean reward (train) = -167.142\tmean reward (valid) = -134.034\n",
            "epoch #05\tmean reward (train) = -132.595\tmean reward (valid) = -173.017\n",
            "epoch #06\tmean reward (train) = -203.250\tmean reward (valid) = -152.431\n",
            "Pendulum принят!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk4y-e5Nm6kc",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3zfYRRlm6kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# record sessions\n",
        "#import gym.wrappers\n",
        "#env = gym.wrappers.Monitor(\n",
        "#    NormalizedActions(gym.make(\"Pendulum-v0\")),\n",
        "#    directory=\"videos_ddpg\", \n",
        "#    force=True)\n",
        "#sessions = [generate_session(t_max=max_steps, train=False) for _ in range(10)]\n",
        "#env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XphEvqcJm6kf",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf8vV8Fym6kf",
        "colab_type": "text"
      },
      "source": [
        "### Задание 1: обучите алгоритм DDPG на одной из сред с непрерывным пространством действий, которые мы рассматривали на одном из прошлых семинаров. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A31NEwU7m6kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRRbFUK-m6kh",
        "colab_type": "text"
      },
      "source": [
        "### Задание 2: Измените гиперпараметры алгоритма так, чтобы получить лучшую кривую сходимости.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRGzhEyXm6ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}