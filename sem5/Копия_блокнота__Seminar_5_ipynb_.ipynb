{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Копия блокнота \"Seminar-5.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsqig2FndZV-",
        "colab_type": "text"
      },
      "source": [
        "## Семинар 5: Proximal Policy Optimization \n",
        "\n",
        "В практической реализации существует два варианта реализации алгоритма PPO:\n",
        "* выполняет обновление, ограниченное KL, как TRPO, но штрафует KL-расхождение в целевой функции вместо того, чтобы делать его жестким ограничением, и автоматически регулирует коэффициент штрафа в процессе обучения, чтобы он масштабировался соответствующим образом.\n",
        "* не содержит в целевой функции члена KL-дивергенции и вообще не имеет ограничения. Вместо этого полагается на специализированный клиппинг \n",
        "\n",
        "<img src=\"https://spinningup.openai.com/en/latest/_images/math/e62a8971472597f4b014c2da064f636ffe365ba3.svg\">\n",
        "\n",
        "Спойлер: клиппинг - не самое главное в PPO, как это могло показаться на первый взгляд. Алгоритм PPO работает во многом и за счет небольших дополнительных улучшений. Подробнее: https://arxiv.org/pdf/2005.12729.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tq_QGZ8eHaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym[Box2D] --force"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbbraEQGdZV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nElNNRDJdZWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjMU8Ud-dZWE",
        "colab_type": "text"
      },
      "source": [
        "### Сеть Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHa3acSidZWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # actor\n",
        "        self.action_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # critic\n",
        "        self.value_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "\n",
        "        state_value = self.value_layer(state)\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OovjnC08dZWG",
        "colab_type": "text"
      },
      "source": [
        "### PPO policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMv0GC1KdZWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo оценка вознаграждений:\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # нормализация вознаграждений:\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "\n",
        "        # конвертация list в tensor\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "        # оптимизация K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # оцениваем старую стратегию:\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # находим отношение стратегий (pi_theta / pi_theta__old):\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Находим Surrogate Loss:\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "            # делаем шаг градиента\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # копируем веса\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMNgVZlldZWJ",
        "colab_type": "text"
      },
      "source": [
        "### Гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfyIqvGOdZWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"LunarLander-v2\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "render = False\n",
        "solved_reward = 200  # останавливаемся если avg_reward > solved_reward\n",
        "log_interval = 20  # печатаем avg reward  в интервале \n",
        "max_episodes = 50000  # количество эпизодов обучения\n",
        "max_timesteps = 500  # максимальное кол-во шагов в эпизоде\n",
        "n_latent_var = 64  # кол-во переменных в скрытых слоях\n",
        "update_timestep = 2000  # обновляем policy каждые n шагов\n",
        "lr = 0.001 # learning rate\n",
        "betas = (0.9, 0.999) # betas для adam optimizer\n",
        "gamma = 0.99  # discount factor\n",
        "K_epochs = 4  # количество эпох обноеления policy\n",
        "eps_clip = 0.1  # clip параметр для PPO\n",
        "random_seed = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG9gX76edZWL",
        "colab_type": "text"
      },
      "source": [
        "### Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQSD8vjudZWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if random_seed:\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "\n",
        "memory = Memory()\n",
        "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
        "print(lr, betas)\n",
        "\n",
        "# переменные для логирования\n",
        "running_reward = 0\n",
        "avg_length = 0\n",
        "timestep = 0\n",
        "\n",
        "# цикл обучения\n",
        "for i_episode in range(1, max_episodes + 1):\n",
        "    state = env.reset()\n",
        "    for t in range(max_timesteps):\n",
        "        timestep += 1\n",
        "\n",
        "        # запускаем policy_old:\n",
        "        action = ppo.policy_old.act(state, memory)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # сохраняем reward and is_terminal:\n",
        "        memory.rewards.append(reward)\n",
        "        memory.is_terminals.append(done)\n",
        "\n",
        "        # делаем обновление\n",
        "        if timestep % update_timestep == 0:\n",
        "            ppo.update(memory)\n",
        "            memory.clear_memory()\n",
        "            timestep = 0\n",
        "\n",
        "        running_reward += reward\n",
        "        if render:\n",
        "            env.render()\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    avg_length += t\n",
        "\n",
        "    # останавливаем обучение если avg_reward > solved_reward\n",
        "    if running_reward > (log_interval * solved_reward):\n",
        "        print(\"########## Принято! ##########\")\n",
        "        torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(env_name))\n",
        "        break\n",
        "\n",
        "    # логирование\n",
        "    if i_episode % log_interval == 0:\n",
        "        avg_length = int(avg_length / log_interval)\n",
        "        running_reward = int((running_reward / log_interval))\n",
        "\n",
        "        print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "        running_reward = 0\n",
        "        avg_length = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv2kiraFdZWO",
        "colab_type": "text"
      },
      "source": [
        "### Задание 1: В данном семинаре вашей задачей будет среда LunarLander-v2, она сложнее базовых сред, поэтому вам будет необходимо изменить некотороые гиперпараметры."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJwQZRvwdZWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQxyU0hdZWQ",
        "colab_type": "text"
      },
      "source": [
        "### Задание 2: Сравните результаты с начальными гиперпараметрами. Как вы считате, что повлияло больше всего?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A6bMAp9dZWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}